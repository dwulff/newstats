---
title: "Sequential analysis"
author: "<table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'><col width='10%'><col width='10%'>
  <tr style='border:none'>
    <td style='display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none' nowrap>
      <a href='https://dwulff.github.io/newstats'>
        <i class='fas fa-clock' style='font-size:.9em;' ></i>
      </a>
      <a href='https://www.dirkwulff.org/'>
        <i class='fas fa-home' style='font-size:.9em;'></i>
      </a>
      <a href='mailto:dirk.wulff@unibas.ch'>
        <i class='fas fa-envelope' style='font-size: .9em;'></i>
      </a>
      <a href='https://www.linkedin.com/in/dirk-wulff-phd-723a7772/'>
        <i class='fab fa-linkedin' style='font-size: .9em;'></i>
      </a>
      <font style='font-style:normal'>| New statistics</font>
    </td>
    <td style='width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none'>
      <img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
    </td>
  </tr></table>"
output:
  html_document:
    css: practical.css
    self_contained: no
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = FALSE, 
                      eval = FALSE, 
                      warning = FALSE)

options(digits = 3)

# Load packages
library(tidyverse)
library(pwr)

# Load packages
#wein <- read_csv("_sessions/LinearModelsIII/1_Data/wein.csv")
#bigmart <- read_csv("_sessions/LinearModelsIII/1_Data/bigmart.csv")
#avocado <- read_csv("TheRBootcamp/1_Data/avocado.csv")
#avocado_cali <- read_csv("TheRBootcamp/1_Data/avocado_cali.csv")
# psi1 <- read_csv("TheRBootcamp/1_Data/psi_exp1.csv")
# psi2 <- read_csv("TheRBootcamp/1_Data/psi_exp2.csv")
```

<p align="center" width="100%">

  <a href="https://www.youtube.com/watch?v=fn7-JZq0Yxs">  
  <img src="image/psi.jpg" alt="Trulli" style="width:100%"></a>
  <br>
  <font style="font-size:10px">Steven Tash as a psi-test subject in Ghostbusters, from 
    <a href="https://www.imdb.com/name/nm0850865/">
      imdb.com
    </a>
  </font>
</p>


# {.tabset}

## Overview

In this practical, you will explore the merits of sequential analysis with a chance, at the end, to help out Bem's to design more effective study designs.

By the end of this practical, you will know how to conduct sequential analysis using a basic example. 

## Tasks

### A - Setup

1. Open the `NewStats` R project. 

2. Open a new R script. Add your name, date, and "Sequential analysis practical" as comments at the beginning of the script. 

```{r, eval = FALSE, echo = TRUE}
## NAME
## DATE
## Sequential analysis practical
```

3. Save the new script as "power_analysis_practical.R" in the `2_code` folder.

4. Load the necessary packages. See below. 

```{r, echo = T}
# Load packages
library(tidyverse)
library(rpact)

```

5. Use the `read_csv()` function to read in `psi_exp_1.csv`, the data of Bem's first experiment. 

```{r, echo = T, message = F, eval = TRUE}
# Read in data
psi <- read_csv(file = "1_data/psi_exp_1.csv")
```

### B - Improper sequential analysis

1. The function below allows you to illustrate the problem of sequential analysis. It simulates incremental data collection of a one-sample t-Test scenario, where the test is carried out for every sample size specified by the argument `stops` and data collection is terminated whenever a test yields a p-value smaller than `alpha`. The default is `stops = 2:100`, which means that beginning with a sample size of 2, the t-Test will be run after every individual observation until the final sample size is reached or the process process aborts because the p-value is smaller than alpha. Run the function definition in the console, so that you can use the function in the next step.   

```{r, echo = TRUE}
# incremental one sample t-test simulation 
simulate_sequential_onesample <- function(stops = 2:100,
                                          alpha = .05,
                                          mu = 0, 
                                          sigma = 1, 
                                          repeats = 1000){
    stops = c(0, stops)
  ps <- numeric(repeats)
  ns <- numeric(repeats)
  for(i in 1:repeats){
    x = rep(NA, max(stops))
    for(j in 1:(length(stops)-1)){
      pos = (stops[j]+1):stops[j+1]
      x[pos] <- rnorm(length(pos), mu, sigma)
      p <- t.test(x)$p.value
      if(p < alpha) break
      }
    ps[i] <- p
    ns[i] <- stops[j+1]
    }
  # out
  cbind("p_value" = ps, "sample_size" = ns)
}

```

2. Run the function using its default arguments and test how frequently the process yields a significant result. Note that the `mu` of the data is 0, which implies that any significant result is a false positive. The function returns a matrix with two columns, one for the p-values and one for the number of samples. 

```{r, echo = TRUE}
# simulate p-values
result <- simulate_sequential_onesample()

# count significant p-values
mean(result[,"p_value"] < .05)
```

2. About 40% of experiments yielded significant results, even though there is no effect in the population. Play around with `alpha` and try to find a value so that for default `stops = 2:100` the overall number of significant results is approximately `.05`. 

```{r, echo = TRUE, eval = FALSE}
#define alpha
alpha <- XX
  
# simulate p-values
result <- simulate_sequential_onesample(alpha = alpha)

# count significant p-values
mean(result[,"p_value"] < alpha)
```

```{r}
#define alpha
alpha <- .004

# simulate p-values
result <- simulate_sequential_onesample(alpha = alpha)

# count significant p-values
mean(result[,"p_value"] < alpha)
```

3. Each individual alpha needs to be a lot smaller, if the overall alpha shall not exceed `.05`. Now keep the alpha at that level and introduce a small effect with `mu = .3`, which is equivalent to `d = .3`, and run the simulation twice. Once with `stops = 2:100` and the alpha value you have just determined, and once with `stops = 100` and `alpha = .05`, so that we can compare sequential and non-sequential analysis approaches. Then evaluate both the number of significant results and the number of samples taken. 

```{r, echo = TRUE, eval = FALSE}
# simulate sequential
result_sequential <- simulate_sequential_onesample(alpha = XX, 
                                                   mu = .3)

# simulate non-sequential
result_nonsequential <- simulate_sequential_onesample(stops = 100, 
                                                      alpha = .05, 
                                                      mu = .3)

# evaluate power
mean(result_sequential[,"p_value"] < XX)
mean(result_nonsequential[,"p_value"] < .05)

# evaluate sample sizes
mean(result_sequential[,"sample_size"])
mean(result_nonsequential[,"sample_size"])
```

```{r}
# simulate sequential
result_sequential <- simulate_sequential_onesample(alpha = .004, 
                                                   mu = .3)

# simulate non-sequential
result_nonsequential <- simulate_sequential_onesample(stops = 100, 
                                                      alpha = .05, 
                                                      mu = .3)

# evaluate power
mean(result_sequential[,"p_value"] < .004)
mean(result_nonsequential[,"p_value"] < .05)

# evaluate sample sizes
mean(result_sequential[,"sample_size"])
mean(result_nonsequential[,"sample_size"])
```

4. The result is kind of mixed. The probability to detect the effect (aka power) is smaller, when using smaller, constant alpha levels in sequential analysis. However, in return, we see that the still pretty sizable level of power in the sequential analysis was achieved with only 70% of the samples. Numbers may vary depending on the level of `alpha`. So this kind of sequential analysis is neither really bad, nor a real improvement. What do you think how could we do better. 


### C - Proper sequential analysis

1. Before we start running better types of sequential analyses, let us define an adapted simulation function that, in place of the old `alpha` argument, has a new argument `alphas`, which expects a numeric of the same length as `stops` specifying the level of alpha used for each individual stop. Run the function definition, so that you can use it in the next step.    

```{r, echo = TRUE}
# incremental one sample t-test simulation 
simulate_sequential_proper <- function(stops,
                                       alphas,
                                       mu = 0, 
                                       sigma = 1, 
                                       repeats = 1000){
  if(length(stops) != length(alphas)) stop("stops and alphas need to be equally long.")
  stops = c(0, stops)
  ps <- numeric(repeats)
  ns <- numeric(repeats)
  for(i in 1:repeats){
    x = rep(NA, max(stops))
    for(j in 1:(length(stops)-1)){
      pos = (stops[j]+1):stops[j+1]
      x[pos] <- rnorm(length(pos), mu, sigma)
      p <- t.test(x)$p.value
      if(p < alphas[j]) break
      }
    ps[i] <- p < alphas[j] 
    ns[i] <- stops[j+1]
    }
  # out
  cbind("signficant" = ps, "sample_size" = ns)
}

```

2. Now take a look at the `getDesignGroupSequential()` function from the `rpact` package. Provided with some necessary inputs this functions determines for you appropriate levels of alpha for each of the testing steps that you would like to perform. Determine alphas for ten steps in a two sided design with a maximum alpha of `.05` by setting `alpha = .05`, `informationRates = seq(.1, 1, .1)`, and `sided = 2`. Print the object and try to make sense of the presented information.    

```{r, echo = TRUE}
# determine alphas
alphas <- getDesignGroupSequential(alpha = .05, 
                                   informationRates = seq(.1, 1, .1), 
                                   sided = 2)

# print
alphas
```

3. One element in the print is `Cumulative alpha spending`. This is the alpha values we should use to optimize the sequential analysis process, at least according to the O'Brien & Fleming approach, which is the default implemented in `getDesignGroupSequential()`. As you can see an apporpiate sequence of alpha values is not composed of constant alpha values, but of very small values in the beginning and increasing larger values towards the maximum sample size planned. Extract this element containing the alpha sequence and store in an object. 

```{r, echo = TRUE}
# determine alphas
alpha_spending <- alphas$alphaSpent
```

4. Now use the `simulate_sequential_proper()` function equipped with `alpha_spending` and `stops = seq(10, 100, 10)`, to evaluate whether we have an false error rate of 5% in the absence of an effect (`mu = 0`). Note that we are not using the same stops, since the function has a maximum number of stops of `20`. Also note that the relevant column in the output of `simulate_sequential_proper()` is now called `"significant"`. 


```{r, echo = TRUE, eval = FALSE}
# simulate sequential
result_sequential <- simulate_sequential_proper(stops = XX,
                                                alphas = XX,
                                                mu = 0)

# evaluate power
mean(result_sequential[,"signficant"])
```

```{r}
# simulate sequential
result_sequential <- simulate_sequential_proper(stops = seq(10, 100, 10),
                                                alphas = alpha_spending,
                                                mu = 0, 
                                                repeats = 10000)

# evaluate power
mean(result_sequential[,"signficant"])
```

5. The false error rate is higher than it should be, but not by much. Let's see how power and sample size has been affected. Use the function to compare again the sequential and the non-sequential approach for a small effect (`mu = .3`), while using `alpha_spending` for the sequential approach. Set alpha of the non-sequntial approach to the false error just determined, so that everything stays comparable. 

```{r, echo = TRUE, eval = FALSE}
# simulate sequential
result_sequential <- simulate_sequential_proper(stops = XX,
                                                alphas = XX,
                                                mu = XX)

# simulate sequential
result_nonsequential <- simulate_sequential_proper(stops = XX,
                                                alphas = XX,
                                                mu = XX)

# evaluate power
mean(result_sequential[,"signficant"])
mean(result_nonsequential[,"signficant"])

# evaluate power
mean(result_sequential[,"sample_size"])
mean(result_nonsequential[,"sample_size"])
```

```{r}
# simulate sequential
result_sequential <- simulate_sequential_proper(stops = seq(10, 100, 10),
                                                alphas = alpha_spending,
                                                mu = .3)

# simulate sequential
result_nonsequential <- simulate_sequential_proper(stops = 100,
                                                alphas = .0673,
                                                mu = .3)

# evaluate power
mean(result_sequential[,"signficant"])
mean(result_nonsequential[,"signficant"])

# evaluate power
mean(result_sequential[,"sample_size"])
mean(result_nonsequential[,"sample_size"])
```

6. Power is now pretty comparable, maybe still tad smaller, but certainly much higher than when using a constant reduced alpha.  however there is a substantial difference in the number of samples used to achieve this level of power. Specifically, the sequential approach still relies on average on only about 70% of the samples. Feel free to play around with the `getDesignGroupSequential()` function. For instance try out different methods to determine the alpha sequence using the `type` argument.  

### X - Back to Bem

1. Remember the number of samples that Bem needed to detect a small effect of `d = .1`? Try to use the examples above to determine how many samples Bem could have saved by using a sequential analysis approach. 


## Data

| Name | Description |
|:-------------|:-------------------------------------|
| `time` | Hour of day |
| `gender` | Gender of participant |
| `age` | Age of participant |
| `presented_gender` | Gender of person on presented picture |
| `erotic` | Hit rate on erortic pictures |
| `control` | Hit rate on control pictures |
| `stimulus_seeking` | Level of stimulus seeking of person. |

## Resources

[**Overview**](https://www.researchgate.net/profile/Stephen-Poon-5/post/Totally-Different-Alpha-values-for-the-same-questionnaire-from-different-Cities-sample-Should-i-use-data-with-low-Alpha-values-for-final-analysis/attachment/59d6488f79197b80779a33ae/AS%3A467386026467330%401488444920245/download/INTERIM+analysis.pdf) on the alpha sending function approach by DeMets and Lan (1994).<br>
[**Tutorial**](https://psyarxiv.com/x4azm/download?format=pdf) on group sequential designs by Lakens and colleagus (2021) 

