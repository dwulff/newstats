---
title: "Bayesian analysis"
author: "<table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'><col width='10%'><col width='10%'>
  <tr style='border:none'>
    <td style='display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none' nowrap>
      <a href='https://dwulff.github.io/newstats'>
        <i class='fas fa-clock' style='font-size:.9em;' ></i>
      </a>
      <a href='https://www.dirkwulff.org/'>
        <i class='fas fa-home' style='font-size:.9em;'></i>
      </a>
      <a href='mailto:dirk.wulff@unibas.ch'>
        <i class='fas fa-envelope' style='font-size: .9em;'></i>
      </a>
      <a href='https://www.linkedin.com/in/dirk-wulff-phd-723a7772/'>
        <i class='fab fa-linkedin' style='font-size: .9em;'></i>
      </a>
      <font style='font-style:normal'>| New statistics</font>
    </td>
    <td style='width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none'>
      <img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
    </td>
  </tr></table>"
output:
  html_document:
    css: practical.css
    self_contained: no
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = FALSE, 
                      eval = FALSE, 
                      warning = FALSE)

options(digits = 3)

# Load packages
library(tidyverse)
library(pwr)

# Load packages
#wein <- read_csv("_sessions/LinearModelsIII/1_Data/wein.csv")
#bigmart <- read_csv("_sessions/LinearModelsIII/1_Data/bigmart.csv")
#avocado <- read_csv("TheRBootcamp/1_Data/avocado.csv")
#avocado_cali <- read_csv("TheRBootcamp/1_Data/avocado_cali.csv")
# psi1 <- read_csv("TheRBootcamp/1_Data/psi_exp1.csv")
# psi2 <- read_csv("TheRBootcamp/1_Data/psi_exp2.csv")
```

<p align="center" width="100%">

  <a href="https://www.youtube.com/watch?v=fn7-JZq0Yxs">  
  <img src="image/psi.jpg" alt="Trulli" style="width:100%"></a>
  <br>
  <font style="font-size:10px">Steven Tash as a psi-test subject in Ghostbusters, from 
    <a href="https://www.imdb.com/name/nm0850865/">
      imdb.com
    </a>
  </font>
</p>


# {.tabset}

## Overview

<p align ="center">
"The term psi denotes anomalous processes of information or energy transfer that are currently unexplained in terms of known physical or biological mechanisms. Two variants of psi are precognition (conscious cognitive awareness) and premonition (affective apprehension) of a future event that could not otherwise be anticipated through any known inferential process."<br><i>Daryl J. Bem, professor emeritus, Cornell University</i>

</p>

In this practical, you will again analyze the data of Daryl Bem`s imfamous study on human *psi*-abilities to evaluate power and sample planning. 

By the end of this practical, you will know how to conduct and evaluate Bayesian t-tests and regressions. 

## Tasks

### A - Setup

1. Open the `NewStats` R project. 

2. Open a new R script. Add your name, date, and "Power analysis practical" as comments at the beginning of the script. 

```{r, eval = FALSE, echo = TRUE}
## NAME
## DATE
## Power analysis practical
```

3. Save the new script as "power_analysis_practical.R" in the `2_code` folder.

4. Load the necessary packages. See below. 

```{r, echo = T}
# Load packages
library(tidyverse)
library(BayesFactor)
library(bayestestR)
library(rstanarm)
library(bridgesampling)

```

5. Use the `read_csv()` function to read in `psi_exp_1.csv`, the data of Bem's first experiment. 

```{r, echo = T, message = F, eval = TRUE}
# Read in data
psi <- read_csv(file = "1_data/psi_exp_1.csv")
```

6. Print the data set and familiarize yourself with the columns included. You can also use `View()` or `str()`.  


### B - Bayesian t-test

1. Let's begin with the same example as in the previous practical, namely to test whether the average hit rate for `erotic` pictures is higher than 50. Use the `ttestBF()` function to test against $H_0 = 50$ by setting `mu = 50`. Then print the result. Make do you make of it?   

```{r, eval = FALSE, echo = TRUE}
# Bayesian t-test
bayes_ttest = ttestBF(psi$XX, mu = XX)

# print result
bayes_ttest
```

```{r}
# Bayesian t-test
bayes_ttest = ttestBF(psi$erotic, mu = 50)

# print result
bayes_ttest
```

2. The results show a Bayes factor of $B_{10} = 2.18$ for the $H_1$ against $H_0$, which according to Jeffreys (1961) is "no worth more than a bare mention". What does that imply for evidence in favor of the $H_0$? Find out using the code below.

```{r, echo = TRUE}
# Evidence for 0
1/bayes_ttest
```

3. The Bayes factor for $H_0$ against $H_1$ is $B_{01} = 0.46$. Since this value is smaller than 1, we have confirmed that the data favors $H_1$ more than $H_0$, albeit only by a small margin. 

### C - Bayesian t-test: Posterior distributions

1. Another way to look at the t-Test is through the lense of the posterior distribution of the numerator model. Using the `posterior()` function you can generate samples of the posterior distribution. Run it and then print the object. 

```{r, echo = TRUE}
# generate samples of posterior 
chains <- posterior(bayes_ttest, iterations = 5000)

# print chains
chains

```

2. The two columns of interest are `mu` and ` sig2`, the mean and variance of the $H_1$ model. Use `plot()` to inspect the chain and distribution of the posterior samples. See below. The plots labeled `trace` show you the sampling process from the posterior. When you solid black horizontal region then all good. The plots labeled `density` is what we are really interested in. They show our posterior distribution of the model parameters after integrating evidence from the data. Try to make sense of the two distributions, especially the one of `mu`.  

```{r, echo = TRUE}
# show sample of posteriors
plot(chains[,1:2])
```

3. The `mu` distribution shows our diversified belief in the location of the mean under $H_1$. We see that most weight is given to values around 53, which is also where the empirical mean lies. Using this distribution, you can now conduct, depending on your school of Bayesian analysis, follow up analyses to further characterize the distribition. For instance, oftentimes people derive a highest density interval (HDI) or credible interval (CI), not to be confused with the frequentist confidence interval. Use the `hdi()` function from the `bayestestR` package to do so. 

```{r, echo = TRUE}
# compute hdi of posterior
hdi(chains[,1])
```

4. Instead of assessing the posterior of the mean, we can use both the posterior of the mean and variance to calculate the posterior of the effect size. See below. Note that HDIs need not be symmetric like frequentist confidence intervals. 

```{r, echo = TRUE}
# posterior of effect size
d_post = (chains[,1] - 50) / sqrt(chains[,2])

# compute hdi of posterior
hdi(d_post)

# plot distribution with hdi
plot(density(d_post))
abline(v = hdi(d_post)[1,3:4])
```

5. Finally, you can use the posterior to derive a Bayesian maxmimum a-posteriori p-value, which compares the posterior density at the maximum of the posterior distribution to the density at the original $H_0$ value. In our case, this means comparing the density at a hit rate at about 53.3 to the density at 50. Use the `p_map()` function to compute the value. Try to make sense of the illustration.  

```{r, echo = TRUE}
# determine the map p-value
p_map(bayes_ttest)

# illustrate map p-value
dens = density(chains[,1])
plot(dens)
map_x = dens$x[which.max(dens$y)]
abline(v = c(50, map_x))
points(c(50, map_x), c(dens$y[which(round(dens$x,2) == 50)[1]], 
                       dens$y[which.max(dens$y)]), pch=16)
```

### D - Bayesian t-test: Role of prior

1. The key prior distribution for the t-Test is the Cauchy distribution, which describes the uncertainty of the `mu` parameter or, to be precised, of the standardized difference between `mu`s (aka *d*). The default scaling parameter is $r = \sqrt{2}/2$, but other values have been proposed, such as $r = 1$ or $r = \sqrt{2}$. The plot in the chunk below illustrates these distributions. Run it and try to make sense of the distributions. 

```{r, echo = TRUE}
# d range
d <- seq(-5, 5, .1)

# priors
plot(d, dcauchy(d, scale = sqrt(2)/2), type = "l", lty = 1)
lines(d, dcauchy(d, scale = 1), lty = 2)
lines(d, dcauchy(d, scale = sqrt(2)), lty = 3)
```

2. Now rerun the t-test for each of the three prior distribitions. What do you observe. 

```{r, eval = F, echo = TRUE}
# t-Tests using all three priors
ttestBF(hitrate_erotic, rscale = XX)
ttestBF(hitrate_erotic, rscale = XX)
ttestBF(hitrate_erotic, rscale = XX)
```

```{r}
# t-Tests using all three priors
ttestBF(hitrate_erotic, mu = 50, rscale = sqrt(2)/2)
ttestBF(hitrate_erotic, mu = 50, rscale = 1)
ttestBF(hitrate_erotic, mu = 50, rscale = sqrt(2))
```

3. Using wider priors has diminished the evidence for the $H_1$ because the wider priors give more weight to extreme effects that are not consistent with the relatively small empirical difference. Many criticism of Bayesian statistics center on the fact that the prior influences the statistical inference. However, the effect of the prior easily washes out with larger sample sizes. Use the code below to replicate the data and then rerun the three tests using the replicated data. What do you observe?

```{r, eval = F, echo = TRUE}
# replicate data
hitrate_erotic_replicated = rep(hitrate_erotic, 2)
```

```{r}
# t-Tests using all three priors
ttestBF(hitrate_erotic_replicated, mu = 50, rscale = sqrt(2)/2)
ttestBF(hitrate_erotic_replicated, mu = 50, rscale = 1)
ttestBF(hitrate_erotic_replicated, mu = 50, rscale = sqrt(2))
```

4. There are still differences between the priors for the replicated, however, all tests show now clear evidence for $H_1$. If you like, try out even higher numbers of replications of the data. You should see that all tests will converge as `N` goes to infinity. 

### E - Bayesian regression

1. In this final section, you will take a look at runnning regressions using the `rstanarm` package. You can also run regressions using `BayesFactor` or other popular packages such as `brms`, but fortunately the implementation across packages is quite analogue. Use the `stan_glm` function to run a regression predicting `erotic` using thr variables `gender` and `stimulus_seeking`. Specify the model as you would do it in a regular `glm()` regression. 

```{r, echo = TRUE, eval = FALSE}
# Run regression
bayes_reg <- stan_glm(formula = XX ~ XX + XX, 
                           data = XX)

```


```{r}
# Run regression
bayes_reg <- stan_glm(formula = erotic ~ gender + stimulus_seeking, 
                           data = psi)

```

2. Before taking a look at the results, have a look at the priors of the model. `rastanarm` offers with `prior_summary()` a neat summary function. Apply it on the fitted `bayes_reg` object and try to make sense of the output. 

```{r, echo = TRUE, eval = FALSE}
# Show priors
prior_summary(XX)
```


```{r}
# Show priors
prior_summary(bayes_reg)
```

3. The output includes several noteworthy elements. First, you can see that all effects (intercept and coefficients) are modeled using normal distributions. Second, the priors are adjusted to reflect the scale of the criterion and the predictors. You can confirm that the scale of the criterion is $2.5 \cdot s_y$, whereas that of the predictors is $2.5 \cdot s_y/s_x$. Third, the location of the intercept prior is not set to 0, but to the mean of the criterion, which is a consquence from centering the predictors (also see corresponding note).

4. Ok, it's time to take a look at the estimates. Simple print the `bayes_reg` object. What do you make of the output?

```{r}
# print
bayes_reg
```

5. The output shows you the medians and the mean absolute deviation of the posterior distributions. This highlights one great thing about having access to posteriors, you can choose whatever statistics you like to summarize it. Here, `rstanarm` expresses a preference for robust statistics. You can also use the standard `summary()` function to obtain some more information. Go ahead and try it out.

```{r}
# summary
summary(bayes_reg)
```

6. The `summary()` shows confidence intervals, as well as some fit and MCMC diagnostics. The posterior predictive uses the posterior to make predictions of the criterion. Essentially it shows how well the criterion is recovered by the regression model. As you can see, the mean is recovered well, but the standard deviation is too small, which you can confirm using `sd(psi$erotic)`. The mcmc diagnostics tell you more about the quality if convergence of the mcmc chains. Poeple often consult $\hat{R}$ (`Rhat`), which compares the estimates of chains across multiple chains. A value of $\hat{R}$ means all is good.      

7. Let's take a look at some posteriors. You can extract them from the the regression object using `as.matrix()`, which will return a matrix samples for each model parameter. These you can then visualize or plug into the `hdi()` function to obtain highest density intervals.  

```{r}
# extract posteriors
posterior <- as.matrix(bayes_reg)

# plot posterior of stimulus_seeking
plot(density(posterior[,"stimulus_seeking"]))

# get hdi
hdi(posterior[,"stimulus_seeking"])

```


8. Finally, `rstanarm` also provides`bayes_R2()`, a function to compute the posterior of $R^2$. Try it out.  

```{r}
# posterior or R-squared
r2_posterior <- bayes_R2(bayes_reg)

# plot
plot(density(r2_posterior))

```


### X - Bayesian regression: Model comparison

1. `rstanarm` does not calculate marginal likelihoods for you. In order to obtain Bayes factors that evaluate the relative evidence for different hypotheses or models we must rely on other packages, such as the `bridgesampler` package. The code below shows you how you can compare two nested models with different predictors. Try it out. Is there more evidence for the model with or without `stimulus_seeking` included?

```{r, echo = TRUE}

# define model one
mod_0 <- stan_glm(formula = erotic ~ gender, data = psi,
                   diagnostic_file = file.path(tempdir(), "df.csv"))

# define model two
mod_1 <- stan_glm(formula = erotic ~ gender + stimulus_seeking, data = psi,
                   diagnostic_file = file.path(tempdir(), "df.csv"))

# obtain samples for model 0
mod_0_samples <- bridge_sampler(mod_0)

# obtain samples for model 1
mod_1_samples <- bridge_sampler(mod_1)

# determine Bayes factor
bayes_factor(mod_1_samples, mod_0_samples)

```


## Data

| Name | Description |
|:-------------|:-------------------------------------|
| `time` | Hour of day |
| `gender` | Gender of participant |
| `age` | Age of participant |
| `presented_gender` | Gender of person on presented picture |
| `erotic` | Hit rate on erortic pictures |
| `control` | Hit rate on control pictures |
| `stimulus_seeking` | Level of stimulus seeking of person. |

## Resources

[**rstanarm vignette**](http://mc-stan.org/rstanarm/articles/rstanarm.html)
[**BayesFactor vignette**](https://richarddmorey.github.io/BayesFactor/)

