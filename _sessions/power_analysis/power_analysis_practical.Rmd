---
title: "Power analysis"
author: "<table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'><col width='10%'><col width='10%'>
  <tr style='border:none'>
    <td style='display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none' nowrap>
      <a href='https://dwulff.github.io/newstats'>
        <i class='fas fa-clock' style='font-size:.9em;' ></i>
      </a>
      <a href='https://www.dirkwulff.org/'>
        <i class='fas fa-home' style='font-size:.9em;'></i>
      </a>
      <a href='mailto:dirk.wulff@unibas.ch'>
        <i class='fas fa-envelope' style='font-size: .9em;'></i>
      </a>
      <a href='https://www.linkedin.com/in/dirk-wulff-phd-723a7772/'>
        <i class='fab fa-linkedin' style='font-size: .9em;'></i>
      </a>
      <font style='font-style:normal'>| New statistics</font>
    </td>
    <td style='width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none'>
      <img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
    </td>
  </tr></table>"
output:
  html_document:
    css: practical.css
    self_contained: no
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = FALSE, 
                      eval = FALSE, 
                      warning = FALSE)

options(digits = 3)

# Load packages
library(tidyverse)
library(pwr)

# Load packages
#wein <- read_csv("_sessions/LinearModelsIII/1_Data/wein.csv")
#bigmart <- read_csv("_sessions/LinearModelsIII/1_Data/bigmart.csv")
#avocado <- read_csv("TheRBootcamp/1_Data/avocado.csv")
#avocado_cali <- read_csv("TheRBootcamp/1_Data/avocado_cali.csv")
# psi1 <- read_csv("TheRBootcamp/1_Data/psi_exp1.csv")
# psi2 <- read_csv("TheRBootcamp/1_Data/psi_exp2.csv")
```

<p align="center" width="100%">

  <a href="https://www.youtube.com/watch?v=fn7-JZq0Yxs">  
  <img src="image/psi.jpg" alt="Trulli" style="width:100%"></a>
  <br>
  <font style="font-size:10px">Steven Tash as a psi-test subject in Ghostbusters, from 
    <a href="https://www.imdb.com/name/nm0850865/">
      imdb.com
    </a>
  </font>
</p>


# {.tabset}

## Overview

<p align ="center">
"The term psi denotes anomalous processes of information or energy transfer that are currently unexplained in terms of known physical or biological mechanisms. Two variants of psi are precognition (conscious cognitive awareness) and premonition (affective apprehension) of a future event that could not otherwise be anticipated through any known inferential process."<br><i>Daryl J. Bem, professor emeritus, Cornell University</i>

</p>

In this practical, you will analyze the data of Daryl Bem`s imfamous study on human *psi*-abilities to evaluate power and sample planning. 

By the end of this practical, you will know how to conduct power analysis using the `pwr` R package and simulations. 

## Tasks

### A - Setup

1. Open the `NewStats` R project. 

2. Open a new R script. Add your name, date, and "Power analysis practical" as comments at the beginning of the script. 

```{r, eval = FALSE, echo = TRUE}
## NAME
## DATE
## Power analysis practical
```

3. Save the new script as "power_analysis_practical.R" in the `2_code` folder.

4. Load the necessary packages. See below. 

```{r, echo = T}
# Load packages
library(tidyverse)
library(pwr)

```

5. Use the `read_csv()` function to read in `psi_exp_1.csv`, the data of Bem's first experiment. 

```{r, echo = T, message = F, eval = TRUE}
# Read in data
psi <- read_csv(file = "1_data/psi_exp_1.csv")
```

6. Print the data set and familiarize yourself with the columns included. You can also use `View()` or `str()`.  

### B - Power analysis: One-sample t-test  

1. Bem's first analysis evaluates whether the hit rate for erotic images is larger than chance, i.e., `50%`. For this condition, Bem observed an average hit rate of `53.13`%-points implying a `3.13`%-points deviation from random expectations. You can use the template below.       

```{r, echo = TRUE, eval = FALSE}
# extract hit rate
hitrate_erotic <- psi %>% pull(erotic)

# determine difference to H0
difference_to_h0 <- mean(XX) - 50

# calculate Cohen's d
d <- difference_to_h0 / sd(XX)
```

```{r, eval = TRUE}
# extract hit rate
hitrate_erotic <- psi %>% pull(erotic)

# determine difference to H0
difference_to_h0 <- mean(hitrate_erotic) - 50

# calculate Cohen's d
d <- difference_to_h0 / sd(hitrate_erotic)
```

2. A Cohen's d of `.25` is generally considered small, but relevant. Now put yourself in the shoes of a rival researcher, who wants to replicate Bem's effect of this size assuming an expected false positive rate of $\alpha = .05$, a power of $power = 1-\beta = .95$, and a directed alternative (`alternative = "greater"`). How large a sample is necessary? 

```{r, echo = TRUE, eval = FALSE}
# determine sample size
sample_size <- pwr.t.test(d = XX, 
                          sig.level = XX, 
                          power = XX,
                          alternative = "XX",
                          type = "one.sample")
sample_size
```

```{r}
# determine sample size
sample_size <- pwr.t.test(d = .25, 
                          sig.level = .05, 
                          power = .95,
                          alternative = "greater",
                          type = "one.sample")
sample_size
```

3. According to the power analysis, a sample of 175 individuals would have been necessary. Visualize this result using the `plot.power.htest()` function. You can simply apply it on the `sample_size` object. 

```{r}
# Visualize power
plot.power.htest(sample_size) 
```

4. Using the plot you can also evaluate the power that Bem's original test had. There were `r length(hitrate_erotic)` participants in his first study. Look for the corresponding point on the x-axis of your plot and then move up to identify the power.   

5. To determine the power of a completed study based on the empirical effect size is called a post-hoc power analysis. The `pwr.t.test` also allows you to carry out such analyses. Simply specify `n` rather than power. Try it out and determine the exact post-hoc power value for Bem's empirical effect size.  

```{r, echo = TRUE, eval = FALSE}
# Bem's post-hoc power
power <- pwr.t.test(n = XX, 
                    d = .25, 
                    sig.level = .05, 
                    alternative = "greater",
                    type = "one.sample")
power
```

```{r}
# Bem's post-hoc power
power <- pwr.t.test(n = length(hitrate_erotic), 
                    d = .25, 
                    sig.level = .05, 
                    alternative = "greater",
                    type = "one.sample")
power
```

6. Note that post-hoc power analysis shouldn't be confused with a proper power analysis. Power analysis really only makes sense a priori. Besides post-hoc power analysis ignores the fact that the empirical effect size is a potentially unreliable estimate. Nevertheless it can sometimes be illustrative to consider what power level was present for a given effect size in a completed study. For instance, Bem might have reasoned prior to conducting any studies that the effect of *psi*-abilities is likely very small, possibly not larger than `d = .1`. Evaluate for Bem what power his design had to detect effects of that magnitude.      

```{r, echo = TRUE, eval = FALSE}
# Bem's post-hoc power for very small effects
power <- pwr.t.test(n = XX, 
                    d = XX, 
                    sig.level = .05, 
                    alternative = "greater",
                    type = "one.sample")
power
```

```{r}
# Bem's post-hoc power for very small effects
power <- pwr.t.test(n = length(hitrate_erotic), 
                    d = .1, 
                    sig.level = .05, 
                    alternative = "greater",
                    type = "one.sample")
power
```
   
7. For very small effects Bem's study only had a power of `25.7`%. So, given that Bem couldn't know that his effect would be as large as `d = .25`, he should probably have conducted a larger study? How large should his sample have been to detect effects of size `d = .1` with power `.95`.     


```{r, eval = TRUE, echo  = FALSE}
# Determine sample size for very small effects
sample_size <- pwr.t.test(d = .1, 
                        sig.level = .05, 
                        power = .95, 
                        alternative = "greater",
                    type = "one.sample")
sample_size
```

```{r}
# Determine sample size for very small effects
sample_size <- pwr.t.test(d = .1, 
                        sig.level = .05, 
                        power = .95, 
                        alternative = "greater",
                    type = "one.sample")
sample_size
```


8. To be able to confidently (`power = .95`) detect very small effects using $\alpha = .05$, Bem should have conducted a study with `r ceiling(sample_size$n)` participants, that is more than ten times as many as he collected for his actual study 1. Now let's consider Bem would have wanted to be sure that he is not committing a Type 1 error (false positive) by setting $\alpha = .005$, the level suggest in a recent proposal to combat the replication crisis. 

```{r, eval = TRUE, echo= FALSE}
# Determine sample size for very small effects
sample_size <- pwr.t.test(d = .1, 
                        sig.level = .005, 
                        power = .95, 
                        alternative = "greater",
                    type = "one.sample")
sample_size
```

```{r}
# Determine sample size for very small effects
sample_size <- pwr.t.test(d = .1, 
                        sig.level = .005, 
                        power = .95, 
                        alternative = "greater",
                    type = "one.sample")
sample_size
```


9. For this stricter test, Bem would have required a sample of `r ceiling(sample_size$n)` participants. Good thing Bem didn't need to worry about false positives, given the substantial effect that human *psi*-abilities produce for erotic images. 

### C - Power analysis: Alternative analyses

1. It is important to note that power calculations depend on the hypothesis and corresponding analysis. To see this, let's suppose that Bem actually didn't know whether `erotic` images would increase or decrease predictive accuracy, implying an undirected statistical test. Determine the sample size needed for a small effect of `d = .1` in either direction (i.e., `alternative = two.sided`), assuming $\alpha = .05$ and $\beta = .05$. 

```{r}
# Determine sample size for paired design
sample_size <- pwr.t.test(d = .1, 
                        sig.level = .05, 
                        power = .95, 
                        alternative = "two.sided",
                        type = "one.sample")
sample_size
```

2. For a two-sided test, roughly an additional 300 participants are required to obtain the same level of power. The increase is caused by $\alpha$ being split in half so that there are rejection areas corresponding to $\alpha/2$ in either directions. Hence, determining sample sizes for two-sided tests is equivalent to testing one-sided with $\alpha/2$. Confirm this by setting `type` back to `"greater"` and by using $\alpha = .025$.  

```{r}
# Determine sample size for paired design
sample_size <- pwr.t.test(d = .1, 
                        sig.level = .025, 
                        power = .95, 
                        alternative = "greater",
                        type = "one.sample")
sample_size
```

3. Now let's assume, Bem actually wanted to test whether there is a difference in the hit rate for `erotic` between men and women. Let's assume the the expected effect size is again `d = .1`, what is the sample needed for $alpha = .05$ and $power = .95$. To reflect the difference in design set `type = "two.sample"` in the `pwr.t.test()` function.  

```{r}
# Determine sample size for paired design
sample_size <- pwr.t.test(d = .1, 
                        sig.level = .05, 
                        power = .95, 
                        alternative = "greater",
                        type = "two.sample")
sample_size
```

4. For the two-sample test a sample of about twice the size of the sample required for the one sample test would have been necessary. Why do you think is that the case?


### D - Power analysis: Simulation

1. There are situations where it is not straightforward to determine power. This can be because the appropriate effect sizes are not intuitive, the design is complex with imbalanced sampling of cells, one expects violations of assumptiosn, or, for instance, when using mixed models. In those cases, one must rely on simulations to determine power. Such simulations specify a plausible population model and then observe, how often a test is able to detect a true effect. The function below does this for a two sample t-test. Per default, it assumes normal distributions with mean 0 (`mus`) and standard deviation 1 (`sigmas`) and samples sizes of 100 (`ns`). Using these settings, the function than takes repeated random samples and stores the p-value associated with a two-sample t-test. It returns a vector containing the p-value for each repeat. Run the function definition in the console, so that you can use the function in the next task.

```{r, echo = TRUE}
# define t-test simulation function
simulate_two_sample <- function(ns = c(100, 100), 
                                mus = c(0, 0), 
                                sigmas = c(1, 1),
                                var_equal = TRUE, 
                                repeats = 10000){
  
  # container
  ps <- numeric(repeats)
  
  for(i in 1:repeats){
    
    # sample
    x1 <- rnorm(ns[1], mus[1], sigmas[1])
    x2 <- rnorm(ns[2], mus[2], sigmas[2])
    
    # run t-test
    test <- t.test(x1, x2, var_equal = var_equal)
    
    # store p
    ps[i] <- test$p.value
    
  }
  
  # out
  ps
}

```

2. Use the `simulate_two_sample()` function using its default settings, which implicity assume the absence of an effect. Then test the proportion of significant results assuming $\alpha = .05$. What should the result be? Think about it before you run it.  


```{r, echo = TRUE}
# run rimulation
sim <- simulate_two_sample()

# determine proportion significant results
mean(sim < .05)
```

3. You have confirmed $\alpha$, that is the probability to call a result significant although there is no effect! Now let's introduce an effect by setting `mus = c(0, .1)`, which is equivalent to setting `d = .1`, and rerun the simulation.

```{r, echo = TRUE}
# run rimulation
sim <- simulate_two_sample(mus = c(0, .1))

# determine proportion significant results
mean(sim < .05)
```

4. Given that there now is a true effect, you are observing power, not $\alpha$, and power is rather small. You can confirm this result using the `pwr.t.test()` function. See below.  

```{r, echo = TRUE}
# Confirm result using power analysis
sample_size <- pwr.t.test(n = 100, 
                          d = .1, 
                          sig.level = .05, 
                          alternative = "two.sided",
                          type = "two.sample")
sample_size
```

5. Play around with the `simulate_two_sample()` function. You can increase or decrease sample sizes or change the standard deviations. You can also introduce violations of assumptions. Since `var_equal = TRUE`, the t-Test assumes equal variances. So when the make the standard deviations different, the test will no longer match the underlying population model, resulting in changes to both $\alpha$ and $\beta$.  


## Data

| Name | Description |
|:-------------|:-------------------------------------|
| `time` | Hour of day |
| `gender` | Gender of participant |
| `age` | Age of participant |
| `presented_gender` | Gender of person on presented picture |
| `erotic` | Hit rate on erortic pictures |
| `control` | Hit rate on control pictures |
| `stimulus_seeking` | Level of stimulus seeking of person. |

## Resourcen

[**pwr vignette**](https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html)<br>
[**simr tutorial**](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6646942/) by Brysbaert & Stevens (2018)

